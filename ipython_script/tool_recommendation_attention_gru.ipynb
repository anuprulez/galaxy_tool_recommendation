{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool recommendation with Attention network \n",
    "## (Gated recurrent units Attention neural network with weighted cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import h5py\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config().copy()\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class ToolPredictionAttentionModel():\n",
    "  \n",
    "    def __init__(self, parameters):\n",
    "        self.embedding_size = int(parameters[\"embedding_size\"])\n",
    "        self.gru_units = int(parameters[\"units\"])\n",
    "        self.max_len = parameters[\"max_len\"]\n",
    "        self.dimensions = parameters[\"dimensions\"]\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.class_weights = parameters[\"class_weights\"]\n",
    "        self.spatial_dropout = parameters[\"spatial_dropout\"]\n",
    "        self.recurrent_dropout = parameters[\"recurrent_dropout\"]\n",
    "        self.dropout = parameters[\"dropout\"]\n",
    "        \n",
    "    def weighted_loss(self, class_weights):\n",
    "        \"\"\"\n",
    "        Create a weighted loss function. Penalise the misclassification\n",
    "        of classes more with the higher usage\n",
    "        \"\"\"\n",
    "        weight_values = list(class_weights.values())\n",
    "\n",
    "        def weighted_binary_crossentropy(y_true, y_pred):\n",
    "            # add another dimension to compute dot product\n",
    "            expanded_weights = tf.keras.backend.expand_dims(weight_values, axis=-1)\n",
    "            return tf.keras.backend.dot(tf.keras.backend.binary_crossentropy(y_true, y_pred), expanded_weights)\n",
    "        return weighted_binary_crossentropy\n",
    "\n",
    "    def create_model(self):\n",
    "        sequence_input = tf.keras.layers.Input(shape=(self.max_len,), dtype='int32')\n",
    "        embedded_sequences = tf.keras.layers.Embedding(self.dimensions, self.embedding_size, input_length=self.max_len, mask_zero=True)(sequence_input)\n",
    "        embedded_sequences_dropout = tf.keras.layers.SpatialDropout1D(self.spatial_dropout)(embedded_sequences)\n",
    "        gru = tf.keras.layers.GRU(self.gru_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            activation='elu',\n",
    "            recurrent_dropout=self.recurrent_dropout\n",
    "        )\n",
    "        sample_output, sample_hidden = gru(embedded_sequences_dropout, initial_state=None)\n",
    "        attention = BahdanauAttention(self.gru_units)\n",
    "        context_vector, attention_weights = attention(sample_hidden, sample_output)\n",
    "        dropout = tf.keras.layers.Dropout(self.dropout)(context_vector)\n",
    "        output = tf.keras.layers.Dense(self.dimensions, activation='sigmoid')(dropout)\n",
    "        model = tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),\n",
    "            loss=self.weighted_loss(self.class_weights),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    trained_model = h5py.File(model_path, 'r')\n",
    "    dictionary = json.loads(trained_model.get('data_dictionary').value)\n",
    "    best_parameters = json.loads(trained_model.get('parameters').value)\n",
    "    compatible_tools = json.loads(trained_model.get('compatible_tools').value)\n",
    "    reverse_dictionary = dict((str(v), k) for k, v in dictionary.items())\n",
    "    new_model = ToolPredictionAttentionModel(best_parameters).create_model()\n",
    "    new_model.load_weights(model_path)\n",
    "    return dictionary, reverse_dictionary, class_weights, best_parameters, compatible_tools, new_model\n",
    "\n",
    "\n",
    "model_path = \"data/tool_recommendation_attention_model.hdf5\"\n",
    "dictionary, reverse_dictionary, class_weights, best_parameters, compatible_tools, new_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack trained model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'gatk2_indel_realigner', '2': 'vt_normalize', '3': 'bedtools_multiintersectbed', '4': 'picard_BamIndexStats', '5': 'FeatureFinderMultiplex', '6': 'gff_to_sequence', '7': 'EMBOSS: fuzzpro38', '8': 'tp_unfold_column_tool', '9': 'ncbi_tblastn_wrapper', '10': 'PeptideIndexer', '11': 'EMBOSS: shuffleseq87', '12': 'vcftools_isec', '13': 'Filter1', '14': 'fragmenter', '15': 'secretbt2test', '16': 'gops_coverage_1', '17': 'computeMatrix', '18': 'seq_filter_by_id', '19': 'Remove beginning1', '20': 'macs2_callpeak', '21': 'methtools_calling', '22': 'wc_gnu', '23': 'cshl_find_and_replace', '24': 'methtools_destrand', '25': 'bismark_bowtie', '26': 'bamFilter', '27': 'freebayes', '28': 'peakcalling_macs', '29': 'iuc_pear', '30': 'flexbar', '31': 'cshl_fastx_clipper', '32': 'sam_merge2', '33': 'ncbi_makeblastdb', '34': 'bgchem_fragment_merger', '35': 'picard_ARRG', '36': 'cshl_uniq_tool', '37': 'CONVERTER_bedgraph_to_bigwig', '38': 'tp_tac', '39': 'CONVERTER_bed_gff_or_vcf_to_bigwig_0', '40': 'dt_profiler', '41': 'EMBOSS: newseq59', '42': 'IDMerger', '43': 'bedtools_coveragebed_counts', '44': 'picard_FixMateInformation', '45': 'barchart_gnuplot', '46': 'Grep1', '47': 'gatk2_haplotype_caller', '48': 'Show beginning1', '49': 'ctb_chemfp_mol2fps', '50': 'XTandemAdapter', '51': 'trim_galore', '52': 'get_flanks1', '53': 'tp_sed_tool', '54': 'predict_pipeline', '55': 'ctb_simsearch', '56': 'rseqc_inner_distance', '57': 'flexbar_split_RR_bcs', '58': 'heatmapper_deepTools', '59': 'CONVERTER_gff_to_bed_0', '60': 'HighResPrecursorMassCorrector', '61': 'cshl_sort_header', '62': 'bedtools_bedtobam', '63': 'mergeCols1', '64': 'fastq_groomer', '65': 'smooth_running_window', '66': 'blastxml_to_tabular', '67': 'picard_NormalizeFasta', '68': 'FalseDiscoveryRate', '69': 'tmhmm2', '70': 'picard_CollectInsertSizeMetrics', '71': 'deeptools_computeMatrix', '72': 'deeptools_bamCompare', '73': 'ctb_online_data_fetch', '74': 'wig_to_bigWig', '75': 'rseqc_bam2wig', '76': 'picard_EstimateLibraryComplexity', '77': 'bowtie2', '78': 'deeptools_compute_matrix', '79': 'bwa_wrapper', '80': 'sam_to_bam', '81': 'fasta_filter_by_length', '82': 'infernal_cmsearch', '83': 'seq_filter_by_mapping', '84': 'picard_SamToFastq', '85': 'FidoAdapter', '86': 'tp_awk_tool', '87': 'glimmer_knowlegde-based', '88': 'cufflinks', '89': 'tab2fasta', '90': 'samtools_sort', '91': 'deeptools_heatmapper', '92': 'ctb_remIons', '93': 'cshl_fastx_quality_statistics', '94': 'proteomics_search_protein_prophet_1', '95': 'blockbuster', '96': 'addValue', '97': 'remove_tail.py', '98': 'cshl_fastq_to_fasta', '99': 'sample_seqs', '100': 'fastq_to_fasta_python', '101': 'gatk2_realigner_target_creator', '102': 'deeptools_profiler', '103': 'ctb_compound_convert', '104': 'cshl_cut_tool', '105': 'signalp3', '106': 'vcffilter', '107': 'blockclust', '108': 'cshl_awk_replace_in_column', '109': 'deeptools_bamFingerprint', '110': 'sed_stream_editor', '111': 'comp1', '112': 'deeptools_bigwigCompare', '113': 'bamCoverage_deepTools', '114': 'hisat', '115': 'bedtools_intersectBed', '116': 'wolf_psort', '117': 'gatk2_variant_annotator', '118': 'fastq_to_tabular', '119': 'CONVERTER_bed_to_bgzip_0', '120': 'snpSift_filter', '121': 'samtools_rmdup', '122': 'tp_replace_in_column', '123': 'samtools_mpileup', '124': 'silac_analyzer', '125': 'EMBOSS: geecee41', '126': 'tophat2', '127': 'term_id_vs_term_def', '128': 'naive_variant_caller', '129': 'vcftools_merge', '130': 'samtools_flagstat', '131': 'cshl_grep_tool', '132': 'CONVERTER_interval_to_bgzip_0', '133': 'DatamashOps', '134': 'bcftools_view', '135': 'deseq2_single', '136': 'tp_split_on_column', '137': 'FileFilter', '138': 'bedtools_mergebed', '139': 'convert_bc_to_binary_RY.py', '140': 'scaffold2fasta', '141': 'deeptools_computeGCBias', '142': 'cshl_awk_tool', '143': 'peakcalling_macs14', '144': 'proteomics_search_peptide_prophet_1', '145': 'cshl_sed_tool', '146': 'deeptools_bamCorrelate', '147': 'macs2_bdgcmp', '148': 'glimmer_build-icm', '149': 'gff2bed1', '150': 'gatk2_print_reads', '151': 'DatamashTranspose', '152': 'msconvert3_raw', '153': 'IDConflictResolver', '154': 'cuffcompare', '155': 'bedtools_bamtobed', '156': 'flexbar_no_split', '157': 'gatk2_variant_apply_recalibration', '158': 'trimmomatic', '159': 'deeptools_bamCoverage', '160': 'samtools_stats', '161': 'EMBOSS: fuzztran39', '162': 'picard_CASM', '163': 'tp_replace_in_line', '164': 'sam_bw_filter', '165': 'gops_intersect_1', '166': 'tp_cut_tool', '167': 'hgv_david', '168': 'bedtools_genomecoveragebed', '169': 'cshl_fastq_quality_filter', '170': 'random_lines1', '171': 'tp_sorted_uniq', '172': 'bedtools_sortbed', '173': 'cshl_fastx_trimmer', '174': 'cshl_word_list_grep', '175': 'openms_protein_quantifier', '176': 'XY_Plot_1', '177': 'Summary_Statistics1', '178': 'flexbardsc', '179': 'snpEff', '180': 'methtools_plot', '181': 'openms_id_mapper', '182': 'PicardInsertSize', '183': 'ssake', '184': 'antismash', '185': 'cuffmerge', '186': 'aragorn_trna', '187': 'bismark_bowtie2', '188': 'rm_spurious_events.py', '189': 'ConsensusID', '190': 'ncbi_blastn_wrapper', '191': 'sort1', '192': 'filter_bed_on_splice_junctions', '193': 'extract_aln_ends.py', '194': 'cshl_fastq_quality_boxplot', '195': 'Show tail1', '196': 'gatk2_variant_select', '197': 'bedtools_bamtofastq', '198': 'EMBOSS: transeq101', '199': 'modencode_peakcalling_macs2', '200': 'join1', '201': 'fastq_join', '202': 'gatk2_variant_recalibrator', '203': 'picard_ValidateSamFile', '204': 'Extract genomic DNA 1', '205': 'cor2', '206': 'tp_sort_header_tool', '207': 'prokaryotic_ncbi_submission', '208': 'FileMerger', '209': 'picard_MarkDuplicates', '210': 'blast2go', '211': 'fasta2tab', '212': 'cshl_fastx_artifacts_filter', '213': 'term_id_vs_term_name', '214': 'bedtools_intersectbed_bam', '215': 'allele_counts_1', '216': 'Extract_features1', '217': 'bedtools_coveragebed', '218': 'openms_id_file_converter', '219': 'Paste1', '220': 'rsem_calculate_expression', '221': 'ngsutils_bam_filter', '222': 'bwa_mem', '223': 'eukaryotic_ncbi_submission', '224': 'sam2interval', '225': 'ctb_filter', '226': 'bedtools_genomecoveragebed_bedgraph', '227': 'gatk2_reduce_reads', '228': 'heatmapper', '229': 'vcfallelicprimitives', '230': 'stringtie', '231': 'merge_pcr_duplicates.py', '232': 'get_subontology_from', '233': 'samtool_filter2', '234': 'tp_easyjoin_tool', '235': 'tp_find_and_replace', '236': 'coords2clnt.py', '237': 'ProteinQuantifier', '238': 'CONVERTER_interval_to_bed_0', '239': 'MzTabExporter', '240': 'ctb_change_title', '241': 'proteomics_search_tandem_1', '242': 'ctb_pubchem_download_as_smiles', '243': 'fasta_compute_length', '244': 'bam_to_sam', '245': 'bedtools_unionbedgraph', '246': 'hisat2', '247': 'rmcontamination', '248': 'IDMapper', '249': 'htseq_count', '250': 'fastqc', '251': 'cuffdiff', '252': 'Cut1', '253': 'methtools_filter', '254': 'deseq2', '255': 'piranha', '256': 'htseq-count', '257': 'picard_ReorderSam', '258': 'extract_bcs.py', '259': 'deeptools_bam_coverage', '260': 'regex_replace', '261': 'EMBOSS: water107', '262': 'cummeRbund', '263': 'r_correlation_matrix', '264': 'Add_a_column1', '265': 'Convert characters1', '266': 'gops_subtract_1', '267': 'meme_meme', '268': 'megablast_xml_parser', '269': 'rseqc_infer_experiment', '270': 'augustus', '271': 'p_clip_peaks', '272': 'gops_join_1', '273': 'tp_head_tool', '274': 'cummerbund_to_cuffdiff', '275': 'Grouping1', '276': 'cat1', '277': 'interproscan', '278': 'tp_cat', '279': 'blastxml_to_top_descr', '280': 'flexbar_split_RYYR_bcs', '281': 'rgPicardMarkDups', '282': 'bedtools_intersectbed', '283': 'IDFilter', '284': 'gatk2_base_recalibrator', '285': 'tp_multijoin_tool', '286': 'fastq_quality_trimmer', '287': 'Datamash', '288': 'bed2gff1', '289': 'Remove_ending', '290': 'methtools_dmr', '291': 'subtract_query1', '292': 'gtf2bedgraph', '293': 'deeptools_correctGCBias', '294': 'snpSift_annotate', '295': 'ncbi_blastp_wrapper', '296': 'MSGFPlusAdapter', '297': 'gtf_filter_by_attribute_values_list', '298': 'tp_tail_tool', '299': 'deeptools_plot_heatmap', '300': 'ncbi_rpsblast_wrapper', '301': 'bg_uniq', '302': 'tophat', '303': 'Psortb', '304': 'IDPosteriorErrorProbability', '305': 'Count1', '306': 'bam2wig', '307': 'cshl_multijoin', '308': 'cshl_fastx_nucleotides_distribution', '309': 'CONVERTER_interval_to_bedstrict_0'}\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommendations(tool_sequence, labels, topk=20, max_seq_len=25):\n",
    "    tl_seq = tool_sequence.split(\",\")\n",
    "    last_tool_name = reverse_dictionary[str(tl_seq[-1])]\n",
    "    sample = np.zeros(max_seq_len)\n",
    "    for idx, tool_id in enumerate(tl_seq):\n",
    "        sample[idx] = int(tool_id)\n",
    "    sample_reshaped = np.reshape(sample, (1, max_seq_len))\n",
    "    tool_sequence_names = [reverse_dictionary[str(tool_pos)] for tool_pos in tool_sequence.split(\",\")]\n",
    "    # predict next tools for a test path\n",
    "    prediction = new_model.predict(sample_reshaped, verbose=0)\n",
    "    class_weighs = best_parameters[\"class_weights\"]\n",
    "    weight_val = list(class_weights.values())\n",
    "    weight_val = np.reshape(weight_val, (len(weight_val),))\n",
    "    prediction = np.reshape(prediction, (prediction.shape[1],))\n",
    "    #prediction = prediction * weight_val\n",
    "    prediction = prediction / float(np.max(prediction))\n",
    "    prediction_pos = np.argsort(prediction, axis=-1)\n",
    "    # get topk prediction\n",
    "    topk_prediction_pos = prediction_pos[-topk:]\n",
    "    topk_prediction_pos = [item for item in topk_prediction_pos if item != 0]\n",
    "    topk_prediction_val = [int(prediction[pos] * 100) for pos in topk_prediction_pos]\n",
    "    # read tool names using reverse dictionary\n",
    "    pred_tool_ids = [reverse_dictionary[str(tool_pos)] for tool_pos in topk_prediction_pos]\n",
    "    pred_tool_ids_sorted = dict()\n",
    "    for (tool_pos, tool_pred_val) in zip(topk_prediction_pos, topk_prediction_val):\n",
    "        tool_name = reverse_dictionary[str(tool_pos)]\n",
    "        pred_tool_ids_sorted[tool_name] = tool_pred_val\n",
    "    pred_tool_ids_sorted = dict(sorted(pred_tool_ids_sorted.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    ids_tools = dict()\n",
    "    keys = list(pred_tool_ids_sorted.keys())\n",
    "    tool_seq_name = \",\".join(tool_sequence_names)\n",
    "    print(\"Current tool sequence: \")\n",
    "    print()\n",
    "    print(tool_seq_name)\n",
    "    print()\n",
    "    print(\"Recommended tools for the tool sequence '%s' with their scores in decreasing order:\" % tool_seq_name)\n",
    "    print()\n",
    "    for i in pred_tool_ids_sorted:\n",
    "        print(i + \"(\" + str(pred_tool_ids_sorted[i]) + \"%)\")\n",
    "    for key in pred_tool_ids_sorted:\n",
    "        ids_tools[key] = dictionary[key]\n",
    "    print()\n",
    "    print(\"Tool ids:\")\n",
    "    print()\n",
    "    for i in ids_tools:\n",
    "        print(i + \"(\" + str(ids_tools[i]) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tool sequence: \n",
      "\n",
      "vt_normalize\n",
      "\n",
      "Recommended tools for the tool sequence 'vt_normalize' with their scores in decreasing order:\n",
      "\n",
      "bedtools_genomecoveragebed_bedgraph(100%)\n",
      "deeptools_computeGCBias(99%)\n",
      "msconvert3_raw(99%)\n",
      "samtools_stats(99%)\n",
      "flexbar_no_split(99%)\n",
      "deseq2_single(99%)\n",
      "cshl_awk_tool(99%)\n",
      "methtools_calling(99%)\n",
      "rseqc_infer_experiment(99%)\n",
      "MSGFPlusAdapter(99%)\n",
      "tp_head_tool(99%)\n",
      "tp_sort_header_tool(99%)\n",
      "bedtools_intersectbed_bam(99%)\n",
      "sam2interval(99%)\n",
      "merge_pcr_duplicates.py(99%)\n",
      "deeptools_bamFingerprint(99%)\n",
      "cshl_cut_tool(99%)\n",
      "cuffcompare(99%)\n",
      "rseqc_inner_distance(99%)\n",
      "gatk2_variant_select(99%)\n",
      "\n",
      "Tool ids:\n",
      "\n",
      "bedtools_genomecoveragebed_bedgraph(226)\n",
      "deeptools_computeGCBias(141)\n",
      "msconvert3_raw(152)\n",
      "samtools_stats(160)\n",
      "flexbar_no_split(156)\n",
      "deseq2_single(135)\n",
      "cshl_awk_tool(142)\n",
      "methtools_calling(21)\n",
      "rseqc_infer_experiment(269)\n",
      "MSGFPlusAdapter(296)\n",
      "tp_head_tool(273)\n",
      "tp_sort_header_tool(206)\n",
      "bedtools_intersectbed_bam(214)\n",
      "sam2interval(224)\n",
      "merge_pcr_duplicates.py(231)\n",
      "deeptools_bamFingerprint(109)\n",
      "cshl_cut_tool(104)\n",
      "cuffcompare(154)\n",
      "rseqc_inner_distance(56)\n",
      "gatk2_variant_select(196)\n"
     ]
    }
   ],
   "source": [
    "topk = 20 # set the maximum number of recommendations\n",
    "tool_seq = \"2\" # give tools ids in a sequence and see the recommendations. To know all the tool ids, \n",
    "                     # please print the variable 'reverse_dictionary'\n",
    "compute_recommendations(tool_seq, \"\", topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 25)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 25, 119)           36890     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  [(None, 25, 97), (None, 9 63438     \n",
      "_________________________________________________________________\n",
      "bahdanau_attention_7 (Bahdan ((None, 97), (None, 25, 1 19110     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 310)               30380     \n",
      "=================================================================\n",
      "Total params: 149,818\n",
      "Trainable params: 149,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
