{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool recommendation with Attention network \n",
    "## (Gated recurrent units Attention neural network with weighted cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import h5py\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config().copy()\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class ToolPredictionAttentionModel():\n",
    "  \n",
    "    def __init__(self, parameters):\n",
    "        self.embedding_size = int(parameters[\"embedding_size\"])\n",
    "        self.gru_units = int(parameters[\"units\"])\n",
    "        self.max_len = parameters[\"max_len\"]\n",
    "        self.dimensions = parameters[\"dimensions\"]\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.class_weights = parameters[\"class_weights\"]\n",
    "        \n",
    "    def weighted_loss(self, class_weights):\n",
    "        \"\"\"\n",
    "        Create a weighted loss function. Penalise the misclassification\n",
    "        of classes more with the higher usage\n",
    "        \"\"\"\n",
    "        weight_values = list(class_weights.values())\n",
    "\n",
    "        def weighted_binary_crossentropy(y_true, y_pred):\n",
    "            # add another dimension to compute dot product\n",
    "            expanded_weights = tf.keras.backend.expand_dims(weight_values, axis=-1)\n",
    "            return tf.keras.backend.dot(tf.keras.backend.binary_crossentropy(y_true, y_pred), expanded_weights)\n",
    "        return weighted_binary_crossentropy\n",
    "\n",
    "    def create_model(self):\n",
    "        sequence_input = tf.keras.layers.Input(shape=(self.max_len,), dtype='int32')\n",
    "        embedded_sequences = tf.keras.layers.Embedding(self.dimensions, self.embedding_size, input_length=self.max_len, mask_zero=True)(sequence_input)\n",
    "        gru = tf.keras.layers.GRU(self.gru_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            activation='elu'\n",
    "        )\n",
    "        sample_output, sample_hidden = gru(embedded_sequences, initial_state=None)\n",
    "        attention = BahdanauAttention(self.gru_units)\n",
    "        context_vector, attention_weights = attention(sample_hidden, sample_output)\n",
    "        output = tf.keras.layers.Dense(self.dimensions, activation='sigmoid')(context_vector)\n",
    "        model = tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),\n",
    "            loss=self.weighted_loss(self.class_weights),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    trained_model = h5py.File(model_path, 'r')\n",
    "    dictionary = json.loads(trained_model.get('data_dictionary').value)\n",
    "    best_parameters = json.loads(trained_model.get('parameters').value)\n",
    "    compatible_tools = json.loads(trained_model.get('compatible_tools').value)\n",
    "    reverse_dictionary = dict((str(v), k) for k, v in dictionary.items())\n",
    "    new_model = ToolPredictionAttentionModel(best_parameters).create_model()\n",
    "    new_model.load_weights(model_path)\n",
    "    return dictionary, reverse_dictionary, class_weights, best_parameters, compatible_tools, new_model\n",
    "\n",
    "\n",
    "model_path = \"data/tool_recommendation_attention_model.hdf5\"\n",
    "dictionary, reverse_dictionary, class_weights, best_parameters, compatible_tools, new_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack trained model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'cshl_sort_header', '2': 'CONVERTER_bed_gff_or_vcf_to_bigwig_0', '3': 'meme_meme', '4': 'rmcontamination', '5': 'EMBOSS: water107', '6': 'ctb_pubchem_download_as_smiles', '7': 'interproscan', '8': 'deeptools_bamFingerprint', '9': 'macs2_bdgcmp', '10': 'regex_replace', '11': 'Grouping1', '12': 'sam_bw_filter', '13': 'bedtools_intersectBed', '14': 'sam2interval', '15': 'sort1', '16': 'seq_filter_by_id', '17': 'FileMerger', '18': 'cummeRbund', '19': 'picard_ValidateSamFile', '20': 'bedtools_bamtobed', '21': 'mergeCols1', '22': 'fragmenter', '23': 'ProteinQuantifier', '24': 'modencode_peakcalling_macs2', '25': 'EMBOSS: fuzztran39', '26': 'hgv_david', '27': 'gops_intersect_1', '28': 'vcftools_isec', '29': 'tp_easyjoin_tool', '30': 'Summary_Statistics1', '31': 'tab2fasta', '32': 'CONVERTER_gff_to_bed_0', '33': 'antismash', '34': 'extract_bcs.py', '35': 'rseqc_infer_experiment', '36': 'Paste1', '37': 'aragorn_trna', '38': 'ngsutils_bam_filter', '39': 'bedtools_coveragebed', '40': 'bamCoverage_deepTools', '41': 'gff2bed1', '42': 'fastq_groomer', '43': 'peakcalling_macs14', '44': 'hisat2', '45': 'ctb_simsearch', '46': 'flexbardsc', '47': 'rm_spurious_events.py', '48': 'picard_MarkDuplicates', '49': 'methtools_dmr', '50': 'ncbi_blastp_wrapper', '51': 'bedtools_sortbed', '52': 'gatk2_variant_select', '53': 'bismark_bowtie', '54': 'tp_cat', '55': 'MSGFPlusAdapter', '56': 'deeptools_plot_heatmap', '57': 'subtract_query1', '58': 'Add_a_column1', '59': 'wolf_psort', '60': 'infernal_cmsearch', '61': 'Extract_features1', '62': 'picard_NormalizeFasta', '63': 'freebayes', '64': 'Grep1', '65': 'tp_cut_tool', '66': 'gatk2_base_recalibrator', '67': 'deseq2', '68': 'EMBOSS: fuzzpro38', '69': 'predict_pipeline', '70': 'deseq2_single', '71': 'cshl_cut_tool', '72': 'cshl_awk_replace_in_column', '73': 'flexbar_split_RYYR_bcs', '74': 'XY_Plot_1', '75': 'blastxml_to_top_descr', '76': 'tp_tac', '77': 'samtools_sort', '78': 'naive_variant_caller', '79': 'sed_stream_editor', '80': 'remove_tail.py', '81': 'bgchem_fragment_merger', '82': 'scaffold2fasta', '83': 'IDConflictResolver', '84': 'gatk2_print_reads', '85': 'proteomics_search_tandem_1', '86': 'merge_pcr_duplicates.py', '87': 'hisat', '88': 'cshl_word_list_grep', '89': 'picard_CollectInsertSizeMetrics', '90': 'cat1', '91': 'Psortb', '92': 'snpSift_filter', '93': 'convert_bc_to_binary_RY.py', '94': 'allele_counts_1', '95': 'IDFilter', '96': 'CONVERTER_bedgraph_to_bigwig', '97': 'cshl_fastx_artifacts_filter', '98': 'bowtie2', '99': 'stringtie', '100': 'bcftools_view', '101': 'fastq_quality_trimmer', '102': 'Filter1', '103': 'tp_split_on_column', '104': 'picard_EstimateLibraryComplexity', '105': 'DatamashOps', '106': 'samtools_mpileup', '107': 'seq_filter_by_mapping', '108': 'trim_galore', '109': 'ctb_filter', '110': 'deeptools_bamCoverage', '111': 'picard_FixMateInformation', '112': 'ctb_compound_convert', '113': 'fasta_filter_by_length', '114': 'vt_normalize', '115': 'tp_head_tool', '116': 'gatk2_reduce_reads', '117': 'tp_awk_tool', '118': 'vcftools_merge', '119': 'vcfallelicprimitives', '120': 'gatk2_variant_annotator', '121': 'CONVERTER_interval_to_bed_0', '122': 'gatk2_variant_recalibrator', '123': 'Count1', '124': 'bedtools_multiintersectbed', '125': 'bedtools_bedtobam', '126': 'samtools_rmdup', '127': 'cuffcompare', '128': 'tp_multijoin_tool', '129': 'comp1', '130': 'filter_bed_on_splice_junctions', '131': 'bedtools_bamtofastq', '132': 'fastq_to_tabular', '133': 'bedtools_intersectbed_bam', '134': 'cshl_sed_tool', '135': 'smooth_running_window', '136': 'join1', '137': 'snpSift_annotate', '138': 'sam_merge2', '139': 'FileFilter', '140': 'tmhmm2', '141': 'Remove beginning1', '142': 'proteomics_search_peptide_prophet_1', '143': 'fasta_compute_length', '144': 'wig_to_bigWig', '145': 'methtools_destrand', '146': 'glimmer_build-icm', '147': 'openms_protein_quantifier', '148': 'snpEff', '149': 'deeptools_heatmapper', '150': 'glimmer_knowlegde-based', '151': 'ncbi_blastn_wrapper', '152': 'tp_sort_header_tool', '153': 'Remove_ending', '154': 'signalp3', '155': 'XTandemAdapter', '156': 'FalseDiscoveryRate', '157': 'bamFilter', '158': 'random_lines1', '159': 'bedtools_coveragebed_counts', '160': 'htseq-count', '161': 'get_subontology_from', '162': 'DatamashTranspose', '163': 'piranha', '164': 'Convert characters1', '165': 'tophat2', '166': 'cor2', '167': 'macs2_callpeak', '168': 'bam_to_sam', '169': 'picard_SamToFastq', '170': 'gatk2_indel_realigner', '171': 'trimmomatic', '172': 'cshl_fastx_clipper', '173': 'sam_to_bam', '174': 'bedtools_unionbedgraph', '175': 'peakcalling_macs', '176': 'rsem_calculate_expression', '177': 'ncbi_tblastn_wrapper', '178': 'cshl_multijoin', '179': 'gops_subtract_1', '180': 'picard_BamIndexStats', '181': 'CONVERTER_interval_to_bgzip_0', '182': 'ctb_online_data_fetch', '183': 'picard_ReorderSam', '184': 'proteomics_search_protein_prophet_1', '185': 'ncbi_rpsblast_wrapper', '186': 'cshl_fastx_quality_statistics', '187': 'EMBOSS: geecee41', '188': 'bwa_mem', '189': 'bedtools_genomecoveragebed_bedgraph', '190': 'openms_id_file_converter', '191': 'Cut1', '192': 'msconvert3_raw', '193': 'tp_replace_in_column', '194': 'heatmapper_deepTools', '195': 'cummerbund_to_cuffdiff', '196': 'silac_analyzer', '197': 'p_clip_peaks', '198': 'bed2gff1', '199': 'fasta2tab', '200': 'Extract genomic DNA 1', '201': 'rgPicardMarkDups', '202': 'flexbar_split_RR_bcs', '203': 'gatk2_realigner_target_creator', '204': 'deeptools_bamCompare', '205': 'gops_coverage_1', '206': 'methtools_plot', '207': 'gtf2bedgraph', '208': 'bam2wig', '209': 'bwa_wrapper', '210': 'gtf_filter_by_attribute_values_list', '211': 'flexbar', '212': 'cshl_fastx_trimmer', '213': 'htseq_count', '214': 'tp_find_and_replace', '215': 'HighResPrecursorMassCorrector', '216': 'dt_profiler', '217': 'ssake', '218': 'cshl_grep_tool', '219': 'barchart_gnuplot', '220': 'term_id_vs_term_def', '221': 'tp_sed_tool', '222': 'deeptools_computeMatrix', '223': 'tp_tail_tool', '224': 'IDMerger', '225': 'deeptools_profiler', '226': 'CONVERTER_interval_to_bedstrict_0', '227': 'cuffmerge', '228': 'IDMapper', '229': 'deeptools_bam_coverage', '230': 'Show beginning1', '231': 'deeptools_bamCorrelate', '232': 'samtools_flagstat', '233': 'coords2clnt.py', '234': 'cshl_fastx_nucleotides_distribution', '235': 'extract_aln_ends.py', '236': 'wc_gnu', '237': 'fastq_join', '238': 'cshl_fastq_to_fasta', '239': 'blast2go', '240': 'ConsensusID', '241': 'openms_id_mapper', '242': 'ncbi_makeblastdb', '243': 'get_flanks1', '244': 'gff_to_sequence', '245': 'prokaryotic_ncbi_submission', '246': 'samtools_stats', '247': 'eukaryotic_ncbi_submission', '248': 'r_correlation_matrix', '249': 'samtool_filter2', '250': 'methtools_calling', '251': 'tophat', '252': 'ctb_remIons', '253': 'cshl_find_and_replace', '254': 'augustus', '255': 'PicardInsertSize', '256': 'fastq_to_fasta_python', '257': 'bedtools_intersectbed', '258': 'deeptools_correctGCBias', '259': 'tp_unfold_column_tool', '260': 'bismark_bowtie2', '261': 'vcffilter', '262': 'bg_uniq', '263': 'rseqc_inner_distance', '264': 'ctb_chemfp_mol2fps', '265': 'fastqc', '266': 'deeptools_bigwigCompare', '267': 'PeptideIndexer', '268': 'picard_CASM', '269': 'cshl_awk_tool', '270': 'CONVERTER_bed_to_bgzip_0', '271': 'MzTabExporter', '272': 'cufflinks', '273': 'iuc_pear', '274': 'EMBOSS: newseq59', '275': 'secretbt2test', '276': 'EMBOSS: shuffleseq87', '277': 'tp_replace_in_line', '278': 'cshl_fastq_quality_filter', '279': 'blockclust', '280': 'bedtools_mergebed', '281': 'flexbar_no_split', '282': 'rseqc_bam2wig', '283': 'blockbuster', '284': 'Show tail1', '285': 'addValue', '286': 'FeatureFinderMultiplex', '287': 'gops_join_1', '288': 'term_id_vs_term_name', '289': 'IDPosteriorErrorProbability', '290': 'bedtools_genomecoveragebed', '291': 'Datamash', '292': 'gatk2_haplotype_caller', '293': 'FidoAdapter', '294': 'tp_sorted_uniq', '295': 'cuffdiff', '296': 'megablast_xml_parser', '297': 'gatk2_variant_apply_recalibration', '298': 'heatmapper', '299': 'cshl_fastq_quality_boxplot', '300': 'ctb_change_title', '301': 'methtools_filter', '302': 'computeMatrix', '303': 'cshl_uniq_tool', '304': 'blastxml_to_tabular', '305': 'sample_seqs', '306': 'deeptools_compute_matrix', '307': 'deeptools_computeGCBias', '308': 'picard_ARRG', '309': 'EMBOSS: transeq101'}\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recommendations(tool_sequence, labels, topk=20, max_seq_len=25):\n",
    "    tl_seq = tool_sequence.split(\",\")\n",
    "    last_tool_name = reverse_dictionary[str(tl_seq[-1])]\n",
    "    sample = np.zeros(max_seq_len)\n",
    "    for idx, tool_id in enumerate(tl_seq):\n",
    "        sample[idx] = int(tool_id)\n",
    "    sample_reshaped = np.reshape(sample, (1, max_seq_len))\n",
    "    tool_sequence_names = [reverse_dictionary[str(tool_pos)] for tool_pos in tool_sequence.split(\",\")]\n",
    "    # predict next tools for a test path\n",
    "    prediction = new_model.predict(sample_reshaped, verbose=0)\n",
    "    class_weighs = best_parameters[\"class_weights\"]\n",
    "    weight_val = list(class_weights.values())\n",
    "    weight_val = np.reshape(weight_val, (len(weight_val),))\n",
    "    prediction = np.reshape(prediction, (prediction.shape[1],))\n",
    "    #prediction = prediction * weight_val\n",
    "    prediction = prediction / float(np.max(prediction))\n",
    "    prediction_pos = np.argsort(prediction, axis=-1)\n",
    "    # get topk prediction\n",
    "    topk_prediction_pos = prediction_pos[-topk:]\n",
    "    topk_prediction_pos = [item for item in topk_prediction_pos if item != 0]\n",
    "    topk_prediction_val = [int(prediction[pos] * 100) for pos in topk_prediction_pos]\n",
    "    # read tool names using reverse dictionary\n",
    "    pred_tool_ids = [reverse_dictionary[str(tool_pos)] for tool_pos in topk_prediction_pos]\n",
    "    pred_tool_ids_sorted = dict()\n",
    "    for (tool_pos, tool_pred_val) in zip(topk_prediction_pos, topk_prediction_val):\n",
    "        tool_name = reverse_dictionary[str(tool_pos)]\n",
    "        pred_tool_ids_sorted[tool_name] = tool_pred_val\n",
    "    pred_tool_ids_sorted = dict(sorted(pred_tool_ids_sorted.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    ids_tools = dict()\n",
    "    keys = list(pred_tool_ids_sorted.keys())\n",
    "    tool_seq_name = \",\".join(tool_sequence_names)\n",
    "    print(\"Current tool sequence: \")\n",
    "    print()\n",
    "    print(tool_seq_name)\n",
    "    print()\n",
    "    print(\"Recommended tools for the tool sequence '%s' with their scores in decreasing order:\" % tool_seq_name)\n",
    "    print()\n",
    "    for i in pred_tool_ids_sorted:\n",
    "        print(i + \"(\" + str(pred_tool_ids_sorted[i]) + \"%)\")\n",
    "    for key in pred_tool_ids_sorted:\n",
    "        ids_tools[key] = dictionary[key]\n",
    "    print()\n",
    "    print(\"Tool ids:\")\n",
    "    print()\n",
    "    for i in ids_tools:\n",
    "        print(i + \"(\" + str(ids_tools[i]) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tool sequence: \n",
      "\n",
      "CONVERTER_bed_gff_or_vcf_to_bigwig_0\n",
      "\n",
      "Recommended tools for the tool sequence 'CONVERTER_bed_gff_or_vcf_to_bigwig_0' with their scores in decreasing order:\n",
      "\n",
      "get_flanks1(100%)\n",
      "ctb_filter(99%)\n",
      "modencode_peakcalling_macs2(97%)\n",
      "bam_to_sam(96%)\n",
      "deeptools_heatmapper(95%)\n",
      "rmcontamination(95%)\n",
      "bedtools_intersectBed(94%)\n",
      "Show tail1(93%)\n",
      "flexbar_split_RYYR_bcs(93%)\n",
      "gatk2_variant_select(92%)\n",
      "flexbardsc(92%)\n",
      "r_correlation_matrix(92%)\n",
      "DatamashTranspose(91%)\n",
      "deeptools_bamFingerprint(91%)\n",
      "eukaryotic_ncbi_submission(91%)\n",
      "tp_awk_tool(91%)\n",
      "term_id_vs_term_def(91%)\n",
      "computeMatrix(90%)\n",
      "silac_analyzer(90%)\n",
      "cshl_multijoin(90%)\n",
      "\n",
      "Tool ids:\n",
      "\n",
      "get_flanks1(243)\n",
      "ctb_filter(109)\n",
      "modencode_peakcalling_macs2(24)\n",
      "bam_to_sam(168)\n",
      "deeptools_heatmapper(149)\n",
      "rmcontamination(4)\n",
      "bedtools_intersectBed(13)\n",
      "Show tail1(284)\n",
      "flexbar_split_RYYR_bcs(73)\n",
      "gatk2_variant_select(52)\n",
      "flexbardsc(46)\n",
      "r_correlation_matrix(248)\n",
      "DatamashTranspose(162)\n",
      "deeptools_bamFingerprint(8)\n",
      "eukaryotic_ncbi_submission(247)\n",
      "tp_awk_tool(117)\n",
      "term_id_vs_term_def(220)\n",
      "computeMatrix(302)\n",
      "silac_analyzer(196)\n",
      "cshl_multijoin(178)\n"
     ]
    }
   ],
   "source": [
    "topk = 20 # set the maximum number of recommendations\n",
    "tool_seq = \"2\" # give tools ids in a sequence and see the recommendations. To know all the tool ids, \n",
    "                     # please print the variable 'reverse_dictionary'\n",
    "compute_recommendations(tool_seq, \"\", topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 25)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 25, 119)           36890     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  [(None, 25, 97), (None, 9 63438     \n",
      "_________________________________________________________________\n",
      "bahdanau_attention_7 (Bahdan ((None, 97), (None, 25, 1 19110     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 310)               30380     \n",
      "=================================================================\n",
      "Total params: 149,818\n",
      "Trainable params: 149,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
